{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a486074-cdb6-428d-aecf-882b2ccaf122",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/Databricks-BR/lab_outubro_2025/refs/heads/main/Includes/images/handson_lab_outubro.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "634751e8-40dd-4d56-be4e-70b555243904",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# 03 - Automatizando com Lakeflow Jobs\n",
    "\n",
    "Nesta seção, você aprenderá a orquestrar tarefas de notebooks e pipelines utilizando o **Lakeflow Jobs** do Databricks.\n",
    "\n",
    "### Objetivos de Aprendizagem\n",
    "\n",
    "Ao final desta lição, você será capaz de:\n",
    "- Explicar o que é um Lakeflow Job e como ele pode automatizar fluxos de trabalho de dados.\n",
    "- Criar um Lakeflow Job com múltiplas tasks, incluindo notebooks e pipelines.\n",
    "- Definir dependências entre tasks para garantir a execução sequencial correta.\n",
    "- Configurar agendamentos para execução automática dos jobs.\n",
    "- Monitorar e gerenciar execuções de jobs no Databricks.\n",
    "\n",
    "Siga os passos abaixo para criar um Lakeflow Job com 3 tasks: um notebook de carga incremental, um pipeline de ingestão e a atualização de um dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01ac3443-ab45-4833-996f-d229fa75ac2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Acesse a interface de Jobs & Pipelines\n",
    "\n",
    "- No menu lateral esquerdo do Databricks, clique em **Jobs & Pipelines** em uma nova aba.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37eaa609-16d2-4c2e-9058-f9bb597e9701",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## B. Crie um novo Job\n",
    "\n",
    "- Clique em **Create** e selecione **Job**.\n",
    "- Altere o nome do Job para `03 - Automatizando com Lakeflow Jobs - <seu_usuário>`\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8bce469-f369-4b08-8ab2-5a9ba5026002",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Adicione a primeira task: Notebook de carga incremental\n",
    "\n",
    "- No campo **Task name**, insira: `Carga Incremental`\n",
    "- Em **Type**, selecione **Notebook**.\n",
    "- Em **Path**, navegue até o notebook `99_incremental` e selecione-o.\n",
    "- Clique em **Create task**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e398bac-1ebf-4d13-9fe7-d59c92f31ba4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### *C. Adicione Arquivos ao Armazenamento em Nuvem\n",
    "\n",
    "   **NOTA:** Assim como no Laboratório, caso não possua conexão com o Github, os dados devem ser inseridos manualmente no Volume. **Nesse caso, não será necessário criar a task `Carga Incremental` acima.**\n",
    "   \n",
    "   Baixe os arquivos da pasta `Files`como ZIP, extrai o arquivo na sua máquina (ou utilize os arquivos extraídos anteriormente), mas dessa vez entre na pasta **`incremental`** e faça o upload das 4 subpastas.\n",
    "   - Para isso, vá em Catálogo -> seu_catalogo -> seu_schema -> raw_data -> Upload to Volume \n",
    "   ![Ellipsis Icon](./Includes/images/upload_arquivos.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba96a77e-4d4e-4902-97c1-794d235d5eff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## D. Adicione a segunda task: Pipeline de ingestão\n",
    "\n",
    "- Clique no botão **+** abaixo da primeira task para adicionar uma nova task dependente.\n",
    "- No campo **Task name**, insira: `Ingestao_Pipeline`\n",
    "- Em **Type**, selecione **Pipeline**.\n",
    "- Em **Pipeline**, selecione `2 - Desenvolvendo um Projeto de Pipeline Simples - <seu usuário/nome>`.\n",
    "- Clique em **Create task**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cca3824e-9c13-4bcd-84d5-b96bbf1194d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## E. Adicione a terceira task: Atualização do Dashboard\n",
    "\n",
    "- Clique no botão **+** abaixo da segunda task para adicionar uma nova task dependente.\n",
    "- No campo **Task name**, insira: `Atualizar Dashboard`\n",
    "- Em **Type**, selecione **Notebook**.\n",
    "- Em **Path**, navegue até `./Includes/Bike_Rental_Business_Insights` e selecione o notebook correspondente.\n",
    "- Clique em **Create task**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b59b7b91-bc9e-4645-bcc3-8a428a0353fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## F. (Opcional) Configure agendamento\n",
    "\n",
    "- Na aba **Schedule**, defina a periodicidade desejada para execução automática do Job.\n",
    "\n",
    "### Opções de Trigger (Agendamento)\n",
    "\n",
    "| Trigger type    | Comportamento                                                                                                   |\n",
    "|-----------------|----------------------------------------------------------------------------------------------------------------|\n",
    "| **Scheduled**   | Executa o job com base em um agendamento de tempo. [Saiba mais](https://docs.databricks.com/en/jobs/scheduled.html) |\n",
    "| **File arrival**| Executa o job quando novos arquivos chegam em um local monitorado do Unity Catalog. [Saiba mais](https://docs.databricks.com/aws/en/jobs/file-arrival-triggers) |\n",
    "| **Continuous**  | Mantém o job sempre em execução, disparando uma nova execução sempre que uma anterior termina ou falha. [Saiba mais](https://docs.databricks.com/en/jobs/continuous.html) |\n",
    "| **None (manual)** | Execuções são disparadas manualmente pelo botão **Run now** ou programaticamente por outras ferramentas de orquestração. [Saiba mais](https://docs.databricks.com/en/jobs/run-now.html) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b994f609-c713-414e-996f-0df3c5d88b04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## G. Salve e execute o Job\n",
    "\n",
    "- Para executar manualmente, clique em **Run Now**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c51de57-dfa1-4b5a-8e91-e020bc400b0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## **Resumo do fluxo:**\n",
    "1. Executa o notebook de carga incremental.\n",
    "2. Executa o pipeline para ingestão dos novos arquivos.\n",
    "3. Atualiza o dashboard com os dados processados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4d0f434-97c1-4eed-8bd1-ee02cd2b9b41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%py\n",
    "display(dbutils.fs.ls(f\"{vol_path}/customers_cdc/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4199e4ee-8828-437a-bae5-189c23590203",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Recursos Adicionais\n",
    "\n",
    "- [Documentação do Lakeflow Jobs](https://docs.databricks.com/aws/en/jobs/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba1d9524-6c20-4206-9517-1acfbca6f580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"blank\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\" target=\"blank\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\" target=\"blank\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\" target=\"blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2338435193527284,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "03 - Automatizando com Lakeflow Jobs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
