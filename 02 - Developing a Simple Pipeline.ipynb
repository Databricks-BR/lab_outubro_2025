{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a486074-cdb6-428d-aecf-882b2ccaf122",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/Databricks-BR/lab_outubro_2025/refs/heads/main/Includes/images/handson_lab_outubro.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43095792-0c61-4733-b597-b7048cf932d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2 - Desenvolvendo um Pipeline Simples\n",
    "\n",
    "Nesta demonstração, criaremos um projeto simples de Lakeflow Declarative Pipeline usando o novo **editor multifile de Pipeline ETL** com SQL declarativo.\n",
    "\n",
    "### Objetivos de Aprendizagem\n",
    "\n",
    "Ao final desta lição, você será capaz de:\n",
    "- Descrever a sintaxe SQL usada para criar um Lakeflow Declarative Pipeline.\n",
    "- Navegar pelo editor multifile de Pipeline ETL do Lakeflow para modificar configurações do pipeline e ingerir o(s) arquivo(s) de fonte de dados brutos.\n",
    "- Criar, executar e monitorar um Lakeflow Declarative Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23d87a10-f250-4735-be4a-32da39b6ad0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SELECIONE COMPUTE CLÁSSICO OU SERVERLESS\n",
    "\n",
    "Antes de executar as células neste notebook, selecione seu cluster de compute **Clássico** ou **Serverless**. O **Serverless** está habilitado por padrão, mas você pode escolher qualquer um dos dois.\n",
    "\n",
    "Siga estas etapas para selecionar o compute desejado:\n",
    "\n",
    "1. No canto superior direito do notebook, clique no menu suspenso de compute.\n",
    "2. Selecione **Serverless** para usar compute serverless, ou selecione seu cluster **Clássico** se preferir.\n",
    "3. Se o cluster clássico não estiver listado:\n",
    "    - Clique em **Mais** no menu suspenso.\n",
    "    - Na janela pop-up **Anexar a um recurso de computação existente**, selecione o cluster desejado no primeiro menu.\n",
    "4. Se o cluster clássico estiver encerrado, reinicie-o:\n",
    "    - No painel de navegação à esquerda, clique com o botão direito em **Compute** e selecione *Abrir em nova guia*.\n",
    "    - Clique no ícone de triângulo ao lado do nome do cluster para iniciá-lo.\n",
    "    - Aguarde até que o cluster esteja em execução e repita os passos acima para selecioná-lo.\n",
    "\n",
    "**NOTA:** Você pode alternar entre **Clássico** e **Serverless** a qualquer momento, conforme a necessidade do laboratório."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a4beb1d-48d3-47ce-808a-e87d8949d5cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Configuração do Laboratório\n",
    "Abra o notebook [00_variaveis]($./00_variaveis) e redefina as variáveis de ambiente, caso necessário.\n",
    "\n",
    "<img src=\"Includes/images/variáveis.png\" alt=\"Variáveis\" style=\"max-width: 200px;\">\n",
    "\n",
    "Execute a célula abaixo para configurar seu ambiente de trabalho para este curso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07efcaae-e602-4c93-bf8a-c0baeec7e7ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c4088b5-b962-46bd-800c-13caaba6831f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Desenvolvendo e Executando um Pipeline Declarativo Lakeflow com o Editor Multifile de Pipeline ETL\n",
    "\n",
    "Este curso inclui um Pipeline Declarativo Lakeflow simples e pré-configurado para explorar e modificar. Nesta seção, iremos:\n",
    "\n",
    "- Explorar o editor multifile de Pipeline ETL e a sintaxe SQL declarativa  \n",
    "- Modificar as configurações do pipeline  \n",
    "- Executar o Pipeline Declarativo Lakeflow e explorar as tabelas streaming e a view materializada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbff69c8-1902-4d16-a70f-aeb55418cf6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Execute a célula abaixo e **copie o caminho** do output para o seu volume **academy.labs_lakeflow.raw_data**. Você precisará deste caminho ao modificar as configurações do seu pipeline.\n",
    "\n",
    "   Este caminho de volume contém os diretórios **customers_cdc**, **maintenance_logs**, **rides** e **weather**, que contêm os arquivos JSON brutos.\n",
    "\n",
    "   **EXEMPLO DE CAMINHO**: `/Volumes/seu_catalog/labs_lakeflow/raw_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee85f925-fdb4-4cb9-a5b5-9b3efc7e32fb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Path to data source files"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(vol_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28c21a10-614c-4168-8e4f-edd62540b57d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Neste curso, temos arquivos iniciais para você usar em seu pipeline. Esta demonstração utiliza a pasta **2 - Desenvolvendo um Projeto de Pipeline Simples**. Para criar um pipeline e adicionar ativos existentes para associá-lo a arquivos de código já disponíveis em seu Workspace (incluindo pastas do Git), complete o seguinte:\n",
    "\n",
    "   a. Na barra de navegação à esquerda, selecione o ícone **Pasta** ![Folder Icon](./Includes/images/folder_icon.png) para abrir a navegação do Workspace.\n",
    "\n",
    "   b. Navegue até a pasta **Build Data Pipelines with Lakeflow Declarative Pipelines** (você pode já estar nela).\n",
    "\n",
    "   c. **(LEIA POR FAVOR)** Para facilitar, abra este mesmo notebook em uma guia separada:\n",
    "\n",
    "    - Clique com o botão direito no notebook na navegação à esquerda.\n",
    "\n",
    "    - Selecione **Abrir em uma Nova Guia**.\n",
    "\n",
    "   d. Na nova guia, clique no **ícone de três pontos (reticências)** ![Ellipsis Icon](./Includes/images/ellipsis_icon.png) na barra de navegação da pasta.\n",
    "\n",
    "   e. Selecione **Criar** → **ETL Pipeline**.\n",
    "\n",
    "   f. Complete a página de criação do pipeline com o seguinte:\n",
    "\n",
    "    - **Nome**: **`2 - Desenvolvendo um Projeto de Pipeline Simples - <seu usuário/nome>`**\n",
    "    - **Catálogo padrão**: Selecione seu catálogo, definido em **catalog_name**  \n",
    "    - **Schema padrão**: Selecione seu **schema** (banco de dados), definido em **schema_name**\n",
    "\n",
    "   g. Selecione **Adicionar ativos existentes**. No pop-up, complete o seguinte:\n",
    "\n",
    "    - **Pasta raiz do pipeline**: Selecione a pasta **lab_outubro_2025** (`/Workspace/Users/seu-nome-de-usuário-lab/lab_outubro_2025`)\n",
    "\n",
    "    - **Caminhos do código-fonte**: Dentro da mesma pasta raiz acima, selecione a pasta **transformations** (`/Workspace/Users/seu-nome-de-usuário-lab/lab_outubro_2025/transformations`)\n",
    "\n",
    "   h. Clique em **Adicionar**. Isso criará um pipeline e associará os arquivos corretos para esta demonstração.\n",
    "\n",
    "   i. Adicione as variáveis catalog e schema para reconhecer o volume criado. Em Settings, procure a sessão **Configuration**, clique em `Edit configuration` e adicione:\n",
    "    - Key: **catalog** / Value: seu catálogo, definido em **catalog_name**  \n",
    "    - Key: **schema** / Value: seu schema, definido em **schema_name**\n",
    "\n",
    "**Exemplo**\n",
    "\n",
    "![Example Demo 2](./Includes/images/demo02_existing_assets.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e398bac-1ebf-4d13-9fe7-d59c92f31ba4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Adicione Arquivos ao Armazenamento em Nuvem\n",
    "\n",
    "1. Após explorar e executar o pipeline seguindo as instruções no arquivo **`01-bronze.sql`**, execute a célula abaixo para adicionar os primeiros arquivos ao seu volume em: `/Volumes/seu_catalogo/seu_schema/raw_data`.\n",
    "\n",
    "   **NOTA:** Se você receber o erro `name 'carga_dos_dados' is not defined`, será necessário executar novamente o script de configuração do laboratório no início deste notebook para criar o objeto `carga_dos_dados`. Isso é necessário para referenciar corretamente o caminho e copiar o arquivo com sucesso.\n",
    "\n",
    "   **NOTA:** Caso não possua conexão com o Github, os dados devem ser inseridos manualmente no Volume.\n",
    "   Baixe os arquivos da pasta `Files`como ZIP, extrai o arquivo na sua máquina, entre na pasta `initial` e faça o upload das 4 subpastas.\n",
    "   - Para isso, vá em Catálogo -> seu_catalogo -> seu_schema -> raw_data -> Upload to Volume \n",
    "   ![Ellipsis Icon](./Includes/images/upload_arquivos.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3934ad20-c9e0-46c6-943c-75c9e89fdd77",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Add a new JSON file to the data source"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "carga_dos_dados('initial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4d0f434-97c1-4eed-8bd1-ee02cd2b9b41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%py\n",
    "display(dbutils.fs.ls(f\"{vol_path}/customers_cdc/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2a75074-56c3-47e1-8128-9b6a06a6aab7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Complete os seguintes passos para visualizar o novo arquivo em seu volume:\n",
    "\n",
    "   a. Selecione o ícone **Catálogo** ![Catalog Icon](./Includes/images/catalog_icon.png) no painel de navegação à esquerda.  \n",
    "   \n",
    "   b. Expanda o seu volume **seu_catalogo.seu_schema.raw_data**.  \n",
    "   \n",
    "   c. Expanda o diretório **customers_cdc**. Você deverá ver 3 arquivos em seu volume: \n",
    "    - **customers_cdc_2025-06-08.parquet** \n",
    "    - **customers_cdc_2025-06-09.parquet**\n",
    "    - **customers_cdc_2025-06-10.parquet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40d2c247-bc7d-4631-bff0-54319d0b21ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Execute a célula abaixo para visualizar os dados no novo arquivo **/customers_cdc/customers_cdc_2025-06-08.parquet**. Observe o seguinte:\n",
    "\n",
    "   - O arquivo **customers_cdc_2025-06-08.parquet** contém novos dados.  \n",
    "   - O arquivo **customers_cdc_2025-06-08.parquet** possui 22 linhas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85e8a21d-dfb0-41c9-b4a7-147bafef4eb3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Preview the 01.json file"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "spark.sql(f'''\n",
    "  SELECT *\n",
    "  FROM PARQUET.`{vol_path}/customers_cdc/customers_cdc_2025-06-08.parquet`\n",
    "''').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8553b3c-c0b8-4f73-83e2-b0e6b6353c4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Execute a célula abaixo para visualizar os dados da pasta **/customers_cdc**. Observe o seguinte:\n",
    "\n",
    "   - O arquivo **customers_cdc_2025-06-08.parquet** possui 22 linhas.\n",
    "   - O arquivo **customers_cdc_2025-06-09.parquet** possui 19 linhas.\n",
    "   - O arquivo **customers_cdc_2025-06-10.parquet** possui 19 linhas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5872e55-f33d-40eb-a1ea-28d6da461e13",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758585230022}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"c3Bhcmsuc3FsKGYnJycKICBTRUxFQ1QgX21ldGFkYXRhLmZpbGVfbmFtZSwqCiAgRlJPTSBQQVJRVUVULmB7dm9sX3BhdGh9L2N1c3RvbWVyc19jZGMvYAonJycpLmRpc3BsYXkoKQ==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewdde5ddb\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewdde5ddb\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewdde5ddb\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewdde5ddb) SELECT `file_name`,COUNT(*) `column_556ad9e1399` FROM q GROUP BY `file_name`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewdde5ddb\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Linhas por arquivo",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "file_name",
             "id": "column_556ad9e1398"
            },
            "y": [
             {
              "column": "*",
              "id": "column_556ad9e1399",
              "transform": "COUNT"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_556ad9e1399": {
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "3e3121ca-c8c3-499e-96aa-dd7cadb311a0",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 17.5,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "file_name",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "file_name",
           "type": "column"
          },
          {
           "alias": "column_556ad9e1399",
           "args": [
            {
             "column": "*",
             "type": "column"
            }
           ],
           "function": "COUNT",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%python\n",
    "spark.sql(f'''\n",
    "  SELECT _metadata.file_name, *\n",
    "  FROM PARQUET.`{vol_path}/customers_cdc/`\n",
    "''').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9aa81f91-b455-43a1-a5ea-88eac78fb718",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Volte para o pipeline criado e abra o arquivo **01-bronze.sql** e selecione **Dry Run** para validar seu pipeline ETL com os arquivos gerados.\n",
    "\n",
    "   Observe a execução do pipeline e perceba o gráfico do pipeline será gerado, indicando a interação e dependências entre as tabelas. Porém nenhuma tabela foi criado e nenhum dado foi carregado.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42ecf963-8a88-4271-99b9-fb20a50ce559",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Ainda no arquivo **01-bronze.sql** e selecione **Run Pipeline** para executar seu pipeline ETL.\n",
    "\n",
    "   Observe a execução do pipeline e perceba que o pipeline lida automativamente com o paralelismo na carga das tabelas.\n",
    "\n",
    "   Clique na caixa da tabela **customers_cdc_raw** para ter mais detalhes da execução dessa tabela. Em **Table Metrics** poderá ver que foram carregadas 60 linhas, assim como o conteúdo dos arquivos utilizados.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fb90ab7-62c3-4d2c-9a6f-61dfc9092ce2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Exploring Your Streaming Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0191beaf-4d3e-4834-a252-f92f61a4603f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Visualize as novas tabelas de streaming e a view materializada em seu catálogo. Complete o seguinte:\n",
    "\n",
    "   a. Selecione o ícone de catálogo ![Ícone de Catálogo](./Includes/images/catalog_icon.png) no painel de navegação à esquerda.\n",
    "\n",
    "   b. Expanda seu catálogo **academy** (ou o nome que você definiu).\n",
    "\n",
    "   c. Expanda o schema **labs_lakeflow** (ou o nome que você definiu). Observe que as duas tabelas de streaming e a view materializada estão corretamente posicionadas em seus schemas.\n",
    "\n",
    "      - **academy.labs_lakeflow.maintenance_logs_raw** (bronze)\n",
    "\n",
    "      - **academy.labs_lakeflow.maintenance_logs** (silver)\n",
    "\n",
    "      - **academy.labs_lakeflow.maintenance_events** (gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c4a4707-fb9a-4ce1-9181-c98ad03891aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Execute a célula abaixo para visualizar os dados na tabela **{catalog_name}.{schema_name}.customers_cdc_raw**. Antes de executar a célula, quantas linhas essa tabela de streaming deve ter?\n",
    "\n",
    "   Observe o seguinte:\n",
    "      - A tabela contém 60 linhas .\n",
    "      - Na coluna **file_name** você pode ver exatamente de qual arquivo as linhas foram ingeridas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6a1ab99-7be9-48f3-a85f-2e5e1cfc2f78",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758588007834}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "View the streaming table"
    }
   },
   "outputs": [],
   "source": [
    "%py\n",
    "spark.sql(f\"\"\"\n",
    "SELECT *\n",
    "FROM {catalog_name}.{schema_name}.customers_cdc_raw;\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c1176f4-1510-4cd9-a9e3-ce97d55ada82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Complete os passos abaixo para visualizar o histórico da tabela de streaming **customers_cdc_raw**:\n",
    "\n",
    "   a. Selecione o ícone **Catálogo** ![Catalog Icon](./Includes/images/catalog_icon.png) no painel de navegação à esquerda.  \n",
    "   \n",
    "   b. Expanda o schema **{catalog_name}.{schema_name}**.  \n",
    "   \n",
    "   c. Clique no ícone de três pontos (reticências) ao lado da tabela **customers_cdc_raw**.  \n",
    "   \n",
    "   d. Selecione **Abrir no Catalog Explorer**.  \n",
    "   \n",
    "   e. No Catalog Explorer, selecione a guia **Histórico**. Observe que um erro pode ser retornado porque visualizar o histórico de uma tabela de streaming requer **SHARED_COMPUTE**. \n",
    "\n",
    "   f. Acima dos seus catálogos à esquerda, selecione seu cluster de computação SQL.\n",
    "\n",
    "   ![Change Compute](./Includes/images/change_compute.png)  \n",
    "   \n",
    "   g. Volte e observe as duas últimas versões da tabela. Observe o seguinte:  \n",
    "   \n",
    "      - Na coluna **Operation**, as duas últimas atualizações foram **STREAMING UPDATE**.  \n",
    "      \n",
    "      - Expanda os valores de **Operation Parameters** das duas últimas atualizações. Note que ambas usam `\"outputMode\": \"Append\"`.  \n",
    "      \n",
    "      - Encontre a coluna **Operation Metrics**. Expanda os valores das duas últimas atualizações. Observe o seguinte:\n",
    "      \n",
    "         - São exibidas várias métricas para a atualização de streaming: **numRemovedFiles, numOutputRows, numOutputBytes e numAddedFiles**.  \n",
    "         \n",
    "         - Nos valores de `numOutputRows`, 60 linhas foram adicionadas na primeira atualização.\n",
    "   \n",
    "   h. Feche o Catalog Explorer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d324f94-c2e0-49de-90da-ce6a79bf7472",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## E. Visualizando Pipelines Declarativos Lakeflow com a UI de Pipelines\n",
    "\n",
    "Após explorar e criar seu pipeline usando o arquivo **01-bronze.sql** nos passos acima, você pode visualizar os pipelines criados em seu workspace através da interface **Pipelines**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "425a3817-92da-4fc9-9733-466d7b806087",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Complete os seguintes passos para visualizar o pipeline que você criou:\n",
    "\n",
    "   a. No painel de navegação principal à esquerda (você pode precisar expandi-lo selecionando o ícone ![Expandir Painel de Navegação](./Includes/images/expand_main_navigation.png) no canto superior esquerdo do seu workspace), clique com o botão direito em **Jobs & Pipelines** e selecione **Abrir Link em Nova Guia**.\n",
    "\n",
    "   b. Isso deve levar você aos pipelines que você criou. Você deve ver seu pipeline **2 - Desenvolvendo um Projeto de Pipeline Simples - <seu usuário/nome>**.\n",
    "\n",
    "   c. Selecione seu **2 - Desenvolvendo um Projeto de Pipeline Simples - <seu usuário/nome>**. Aqui, você pode usar a interface para modificar o pipeline.\n",
    "\n",
    "   d. Selecione o botão **Settings** no topo. Isso o levará para as configurações dentro da interface.\n",
    "\n",
    "   e. Selecione **Schedule** para agendar o pipeline. Selecione **Cancel**, aprenderemos a agendar o pipeline depois.\n",
    "\n",
    "   f. Abaixo do nome do seu pipeline, selecione o menu suspenso com o carimbo de data e hora. Aqui você pode visualizar o **Gráfico do Pipeline** e outras métricas para cada execução do pipeline.\n",
    "\n",
    "   g. Feche a guia da interface do pipeline que você abriu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4199e4ee-8828-437a-bae5-189c23590203",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Recursos Adicionais\n",
    "\n",
    "- [Documentação do Lakeflow Declarative Pipelines](https://docs.databricks.com/aws/pt/dlt/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba1d9524-6c20-4206-9517-1acfbca6f580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"blank\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\" target=\"blank\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\" target=\"blank\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\" target=\"blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2338435193527284,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "02 - Developing a Simple Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
